import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch
import os
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
import argparse
import gc

device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
import re

PROMPTS = {
    "mixed": (
        "The following text may contain adversarial perturbations generated by techniques such as "
        "word misspelling, synonym substitution, and sentence rephrasing. "
        "lease generate N new texts by rephrasing the input text while preserving its original meaning."

        "Do NOT output any explanations, note or steps. Only return JSON objects in the following format:\n"
        "OUTPUT: {{\n"
        "  \"sentence1\": \"First paraphrased version.\",\n"
        "  \"sentence2\": \"Second paraphrased version.\",\n"
        "  \"sentence3\": \"Third paraphrased version.\",\n"
        "  \"sentence4\": \"Fourth paraphrased version.\",\n"
        "  \"sentence5\": \"Fifth paraphrased version.\"\n"
        "}}\n"

        "\nINPUT: {frag} \n"
        "OUTPUT: \n"
    ),
}


class TextDataset(Dataset):
    def __init__(self, text_list):
        self.text_list = text_list

    def __len__(self):
        return len(self.text_list)

    def __getitem__(self, idx):
        return self.text_list[idx], idx


def collate_fn(batch):
    return batch


def run_once(params, tokenizer, model, batch_size=8, num_return_sequences=1):
    """
    Parameters
    ----------
    params : dict
        Dictionary containing keys: dataset, attack_name, model_name, data_type, path.
    tokenizer : PreTrainedTokenizer
        Tokenizer for the text generation model.
    model : PreTrainedModel
        Text generation model (e.g., GPT or LLaMA-based).
    batch_size : int
        Batch size for processing.
    num_return_sequences : int
        Number of generated sequences per input text.
    """
    dataset = params["dataset"]
    attack_name = params["attack_name"]
    model_name = params["model_name"]
    data_type = params["data_type"]
    model_type = params["path"]

    # Load the original adversarial dataa file
    data = pd.read_csv(
        f'./attack_results/{dataset}/{model_type}/{attack_name}/attack_{model_name}_{data_type}.csv')
    text_list = data['perturbed_text'].tolist()

    # Prepare output path
    output_file = f'./attack_results/{dataset}/{model_type}_ours/{attack_name}/attack_{model_name}_{data_type}.csv'
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    first_write = not os.path.exists(output_file)

    dataset = TextDataset(text_list)
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)

    inst_token = 'Fifth paraphrased version.'
    for batch in tqdm(dataloader, desc="Processing Batches"):
        texts, indices = zip(*batch)
        input_prompts = [PROMPTS['mixed'].format(frag=p) for p in texts]

        # Tokenize and move to device
        model_inputs = tokenizer(input_prompts, return_tensors="pt", padding=True, truncation=True).to("cuda")

        with torch.no_grad():
            generated = model.generate(
                model_inputs.input_ids,
                attention_mask=model_inputs.attention_mask,
                max_new_tokens=256,
                do_sample=True,
                temperature=1,
                top_k=100,
                top_p=0.95,
                num_return_sequences=num_return_sequences,
            )
        generated_texts = tokenizer.batch_decode(generated, skip_special_tokens=True)

        batch_results = []
        # Process each sample in the batch
        for i, idx in enumerate(indices):
            row = data.iloc[idx]
            # Include the original row
            batch_results.append({
                "original_text": row['original_text'],
                "perturbed_ori": row['perturbed_text'],
                "perturbed_text": row['perturbed_text'],
                "ground_truth_output": row['ground_truth_output'],
                "result_type": row['result_type']
            })

            # Extract multiple generated paraphrases for the sample
            sample_outputs = generated_texts[i * num_return_sequences:(i + 1) * num_return_sequences]

            cleaned = []
            for response in sample_outputs:
                inst_index = response.find(inst_token)
                response_trimmed = response[inst_index + len(inst_token):].strip()
                matches = re.findall(r'"sentence[1-5]"\s*:\s*"(.*?)"', response_trimmed)
                cleaned.extend(matches[:5])

                # Remove duplicates and empty strings
                cleaned = list({c for c in cleaned if c})

                # Append cleaned paraphrases to results
                for j, rewrite in enumerate(cleaned, start=1):
                    batch_results.append({
                        "original_text": row['original_text'],
                        "perturbed_ori": row['perturbed_text'],
                        "perturbed_text": rewrite,
                        "ground_truth_output": row['ground_truth_output'],
                        "result_type": row['result_type']
                    })

        # Save batch results to CSV
        df_batch = pd.DataFrame(batch_results)
        df_batch.to_csv(output_file, mode='a', index=False, header=first_write)
        first_write = False


def run_once_benign(params, tokenizer, model, batch_size=8, num_return_sequences=1):
    dataset_name = params["dataset"]
    data_type = params["data_type"]
    model_type = params["path"]

    # Load dataset
    if dataset_name == "ethos":
        data_path = f'./dataset/ethos/{data_type}.csv'
    elif dataset_name == "pheme":
        data_path = f'./dataset/PHEME/{data_type}.csv'
    elif dataset_name == "white":
        data_path = f'./dataset/white/{data_type}.csv'
    else:
        raise ValueError(f"Unsupported dataset: {dataset_name}")

    data = pd.read_csv(data_path)
    text_list = data['text'].tolist()

    # Prepare output file
    output_file = f'./attack_results/{dataset_name}/{model_type}_ours/benign/{data_type}.csv'
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    first_write = not os.path.exists(output_file)

    dataset = TextDataset(text_list)
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)

    for batch in tqdm(dataloader, desc="Processing Batches"):
        texts, indices = zip(*batch)
        input_prompts = [PROMPTS['mixed'].format(frag=t) for t in texts]

        model_inputs = tokenizer(input_prompts, return_tensors="pt", padding=True, truncation=True).to("cuda")

        with torch.no_grad():
            generated = model.generate(
                model_inputs.input_ids,
                attention_mask=model_inputs.attention_mask,
                max_new_tokens=256,
                do_sample=True,
                temperature=1,
                top_k=100,
                top_p=0.95,
                num_return_sequences=num_return_sequences,
            )

        generated_texts = tokenizer.batch_decode(generated, skip_special_tokens=True)

        inst_token = 'Fifth paraphrased version.'
        batch_results = []
        for i, idx in enumerate(indices):
            row = data.iloc[idx]
            # Original sample
            batch_results.append({
                "id": row['id'],
                "text": row['text'],
                "label": row['label']
            })

            # Multiple paraphrased outputs
            sample_outputs = generated_texts[i * num_return_sequences:(i + 1) * num_return_sequences]
            cleaned = []
            for response in sample_outputs:
                inst_index = response.find(inst_token)
                response_trimmed = response[inst_index + len(inst_token):].strip()
                matches = re.findall(r'"sentence[1-5]"\s*:\s*"(.*?)"', response_trimmed)
                cleaned.extend(matches[:5])

                # Remove duplicates and empty strings
                cleaned = list({c for c in cleaned if c})

                # Append paraphrases to results
                for j, rewrite in enumerate(cleaned, start=1):
                    batch_results.append({
                        "id": row['id'],
                        "text": rewrite,
                        "label": row['label']
                    })

        # Save batch results
        df_batch = pd.DataFrame(batch_results)
        df_batch.to_csv(output_file, mode='a', index=False, header=first_write)
        first_write = False


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--path', type=str, default='ST')
    args = parser.parse_args()

    file_dir = './model/Llama-3.1-8B-Instruct'
    path = args.path

    param_list = [
        {"path": path, "dataset": "ethos", "attack_name": 'deepwordbug', "model_name": "bert", "data_type": "val"},
        # {"path": path, "dataset": dataset, "data_type": "val"},
    ]

    # -------------------- Model setup --------------------
    tokenizer = AutoTokenizer.from_pretrained(file_dir)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(file_dir, torch_dtype=torch.float16)
    model.to("cuda")
    model.config.pad_token_id = tokenizer.eos_token_id
    batch_size = 8

    # -------------------- Execute text generation --------------------
    for params in param_list:
        print(f"\n==== Running parameters: {params} ====")
        # Generate adversarial samples
        run_once(params, tokenizer=tokenizer, model=model, batch_size=batch_size)
        # To generate benign samples, uncomment the following line:
        # run_once_benign(params, tokenizer=tokenizer, model=model, batch_size=batch_size)
